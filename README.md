# üìÇApresenta√ß√£o

Este reposit√≥rio acompanha meu desenvolvimento na bolsa de estudos ofertada pela Compass UOL em Engenhariaa de Dados em Nuvem(AWS), onde estou desenvolvendo habilidades em tecnologias de dados e computa√ß√£o em nuvem. Durante o programa, estarei trabalhando com ferramentas de engenharia de dados, processamento em cloud, e visualiza√ß√£o de dados. O objetivo √© aplicar essas tecnologias em projetos pr√°ticos e compartilhar os desafios resolvidos, exerc√≠cios, evid√™ncias de progresso, e certificados obtidos ao longo do processo.

## üí´ Sobre Mim

![Foto Minha](Sprint-1/evidencias/minha-foto.jpeg)

- üìç | **Localiza√ß√£o**: Paragominas, Par√°.
- üéì | **Educa√ß√£o**: Estudante de Sistemas de Informa√ß√£o na UFRA - Campus Paragominas, 5¬∫ semestre.
- üíº | **Fun√ß√£o**: Bolsista em Engenharia de Dados em Nuvem na Compass UOL.
- üöÄ | **Foco**: Engenharia de Dados e Programa√ß√£o
- üéÆ | **Hobbies**: Jogar no PS5 e treinar na academia nos tempos livres.

## üåê Redes Sociais

[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?logo=linkedin&logoColor=white)](https://linkedin.com/in/gabrielpereiraplus/)

## üíª Tech Stack

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![MySQL](https://img.shields.io/badge/mysql-4479A1.svg?style=for-the-badge&logo=mysql&logoColor=white) ![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white) ![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white) ![JavaScript](https://img.shields.io/badge/javascript-%23323330.svg?style=for-the-badge&logo=javascript&logoColor=%23F7DF1E) ![HTML5](https://img.shields.io/badge/html5-%23E34F26.svg?style=for-the-badge&logo=html5&logoColor=white)
![CSS3](https://img.shields.io/badge/css3-%231572B6.svg?style=for-the-badge&logo=css3&logoColor=white)

## üöÄ Portf√≥lio de Estudos

### [Sprint 1:](Sprint-1/README.md) Introdu√ß√£o ao Git, GitHub, Markdown, e Fundamentos de Linux

**Resumo:** Na Sprint 1, aprendi a utilizar Git e GitHub para controle de vers√£o e colabora√ß√£o em projetos. Explorei o uso de Markdown para documenta√ß√£o e adquiri habilidades pr√°ticas em Linux, especialmente na manipula√ß√£o de arquivos e automa√ß√£o com Bash.

**Principais Aprendizados:**

- Uso de Git para controle de vers√£o e gerenciamento de reposit√≥rios.
- Documenta√ß√£o com Markdown.
- Fundamentos de Linux e manipula√ß√£o de arquivos.
- Automa√ß√£o de processos com Bash Scripting.

**Desafio:** Cria√ß√£o de scripts para automa√ß√£o de backup de arquivos de vendas, otimizando o fluxo de trabalho e incorporando a data nos nomes dos arquivos.

### [Sprint 2:](Sprint-2/README.md) SQL Aplicado √† An√°lise de Dados e Modelagem de Banco de Dados

**Resumo:** Na Sprint 2, aprofundei meus conhecimentos em SQL, focando em consultas a bancos de dados para extrair e analisar dados relevantes de problemas reais de neg√≥cio. Aprendi a criar dashboards que respondem perguntas-chave usando SQL e a aplicar boas pr√°ticas para otimizar o desempenho das queries.

**Principais Aprendizados:**

- Uso da linguagem SQL para realizar consultas avan√ßadas (queries) em bancos de dados.
- An√°lise de dados de neg√≥cios com SQL, criando insights valiosos para tomadas de decis√£o.
- Boas pr√°ticas de performance, garantindo melhor efici√™ncia e velocidade nas consultas SQL.

**Desafio:** O objetivo foi aplicar os conhecimentos de modelagem de dados relacional e dimensional com SQL. Recebi uma base de dados de loca√ß√µes de ve√≠culos para normalizar, seguindo as formas normais e convertendo o modelo relacional em um modelo dimensional.

### [Sprint 3:](Sprint-3/README.md) Python e Manipula√ß√£o de Dados com Numpy e Matplotlib

**Resumo:** Na Sprint 3, o foco foi o uso de Python para manipula√ß√£o e an√°lise de dados, utilizando bibliotecas como Numpy e Matplotlib. A proposta envolveu a leitura e o processamento de um arquivo de estat√≠sticas da Google Play Store, com o objetivo de gerar visualiza√ß√µes gr√°ficas para facilitar a interpreta√ß√£o dos dados. A sprint refor√ßou a import√¢ncia da visualiza√ß√£o de dados para insights e tomadas de decis√£o.

**Principais Aprendizados:**

- Manipula√ß√£o de arrays e dados num√©ricos com a biblioteca Numpy.
- Cria√ß√£o de gr√°ficos e visualiza√ß√µes de dados com Matplotlib.
- Leitura e processamento de arquivos CSV em Python.
- Utiliza√ß√£o de gr√°ficos para interpreta√ß√£o e an√°lise de dados.

**Desafio:** O desafio consistiu em ler um arquivo de estat√≠sticas da loja do Google, processar os dados utilizando Numpy para realizar c√°lculos relevantes e gerar gr√°ficos com Matplotlib. A tarefa envolveu criar visualiza√ß√µes que exibissem, por exemplo, a distribui√ß√£o das classifica√ß√µes dos aplicativos.

### [Sprint 4:](Sprint-4/README.md)  Programa√ß√£o Funcional e Containers com Docker

**Resumo:** Na Sprint 4, o foco foi aprofundar os conceitos de programa√ß√£o funcional, incluindo o uso de fun√ß√µes puras e imutabilidade, al√©m de introduzir os containers, explorando Docker e Kubernetes. Esse aprendizado envolveu entender a constru√ß√£o e execu√ß√£o de imagens e containers Docker para automa√ß√£o e encapsulamento de scripts Python.

**Principais Aprendizados:**

- Princ√≠pios de programa√ß√£o funcional, como fun√ß√µes puras e imutabilidade.
- Cria√ß√£o e execu√ß√£o de imagens e containers com Docker.
- Configura√ß√£o e utiliza√ß√£o de containers para isolar e automatizar scripts Python.
- Introdu√ß√£o ao Kubernetes para orquestra√ß√£o de containers.

**Desafio:** O desafio envolveu criar e executar um script Python para mascaramento de dados, utilizando Docker para encapsular o c√≥digo e garantir sua portabilidade. Tamb√©m foram realizados testes para verificar a reutiliza√ß√£o de containers, consolidando pr√°ticas de efici√™ncia e automa√ß√£o.

### [Sprint 5:](Sprint-5/README.md) Fundamentos AWS e Automa√ß√£o com Python

**Resumo:** Na Sprint 5, o aprendizado centrou-se em conceitos fundamentais da AWS e em pr√°ticas de automa√ß√£o com Python. As li√ß√µes englobaram desde a manipula√ß√£o de servi√ßos AWS, como S3, at√© o uso de bibliotecas Python, como pandas e polars, para processamento eficiente de dados. A experi√™ncia foi enriquecida pela combina√ß√£o de tarefas pr√°ticas baseadas em casos reais e pela prepara√ß√£o para a certifica√ß√£o AWS Certified Cloud Practitioner (CLF-C02).

**Principais Aprendizados:**

- Conceitos fundamentais da AWS: computa√ß√£o, rede, seguran√ßa, armazenamento e o Modelo de Responsabilidade Compartilhada.
- AWS S3: cria√ß√£o e manipula√ß√£o de buckets, al√©m de upload e download de arquivos.
- Processamento de dados com Python: an√°lise, filtros, agrega√ß√µes e transforma√ß√µes utilizando pandas ou polars.
- Pr√°ticas de automa√ß√£o e integra√ß√£o: pipeline para ingest√£o, processamento e exporta√ß√£o de dados no AWS S3.
- Prepara√ß√£o para certifica√ß√£o: entendimento dos aspectos econ√¥micos da AWS e das pr√°ticas recomendadas de seguran√ßa e custo.

**Desafio:** O desafio consiste em usar AWS S3 e Python para manipular um arquivo CSV do portal de dados p√∫blicos. Ap√≥s analisar os dados localmente, o arquivo deve ser enviado para um bucket no S3, processado com pandas ou polars para aplicar filtros, agrega√ß√µes e transforma√ß√µes, e, por fim, o resultado deve ser salvo como CSV no mesmo bucket.

### [Sprint 6:](Sprint-6/README.md) An√°lise Avan√ßada de Dados na AWS

**Resumo:** Na Sprint 6, o foco foi no aprofundamento das ferramentas anal√≠ticas da AWS, com cursos especializados em EMR, Glue, Redshift, Athena, QuickSight e Analytics. O aprendizado incluiu pr√°ticas avan√ßadas de data warehousing e an√°lise serverless, complementadas por um desafio pr√°tico de constru√ß√£o de um Data Lake. A experi√™ncia pr√°tica envolveu a implementa√ß√£o de um pipeline completo de dados usando Python, Docker e servi√ßos AWS.

**Principais Aprendizados:**

- Execu√ß√£o de consultas SQL em dados do S3 utilizando Amazon Athena
- Estrutura√ß√£o e organiza√ß√£o de Data Lakes na AWS
- Pr√°ticas de governan√ßa e cataloga√ß√£o de dados
- Otimiza√ß√£o de custos em processamento de dados
- Desenvolvimento de pipelines ETL com Python e Docker
- Integra√ß√£o entre diferentes servi√ßos AWS para an√°lise de dados

**Desafio:** O desafio consiste em criar um Data Lake de filmes e s√©ries na AWS, usando S3, Docker e Python. Os dados brutos de arquivos CSV locais s√£o carregados em um bucket S3 com boto3, e o ambiente √© containerizado com Docker. Posteriormente(etapas futuras), Apache Spark ser√° usado para processar os dados e criar um modelo dimensional, culminando na cria√ß√£o de dashboards anal√≠ticos sobre a ind√∫stria cinematogr√°fica.

1. [Sprint 1](Sprint-1/README.md)
2. [Sprint 2](Sprint-2/README.md)
3. [Sprint 3](Sprint-3/README.md)
4. [Sprint 4](Sprint-4/README.md)
5. [Sprint 5](Sprint-5/README.md)
6. [Sprint 6](Sprint-6/README.md)
